
# ğŸ¬ Netflix Data Analysis ELT Pipeline with dbt & Snowflake

This project is a modern data stack implementation of an end-to-end ELT pipeline using **dbt (Data Build Tool)** with **Snowflake** and **AWS S3** as the underlying platforms.

Inspired by [Darshil Parmar's Masterclass Project](https://github.com/darshilparmar/dbt-databuildtool--masterclass-netflix-project), this repo is a fully functional version rebuilt, debugged, and annotated with personal enhancements and documentation for a beginner to grasp the concepts.

---

## ğŸ“ Project Structure

```
Netflix_Data_Analysis/
â”‚
â”œâ”€â”€ netflix_dbt_project/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ dim/
â”‚   â”‚   â””â”€â”€ fct/
â”‚   â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ snapshots/
â”‚   â”œâ”€â”€ seeds/
â”‚   â””â”€â”€ target/
â”‚
â”œâ”€â”€ raw_data/              # CSVs from MovieLens (optional for local copy)
â””â”€â”€ README.md              # This documentation
```

---

## ğŸš€ What This Project Covers

âœ… ELT Pipeline using dbt  
âœ… Data Ingestion from AWS S3 into Snowflake  
âœ… dbt Materializations: view, table, incremental, ephemeral  
âœ… Use of Sources, Seeds, Snapshots, Tests, Macros  
âœ… Project-level data modeling: Staging â†’ Dimensional â†’ Fact  
âœ… dbt testing and documentation generation  

---

## ğŸ“¦ Data Source

- [MovieLens 20M Dataset](https://grouplens.org/datasets/movielens/20m/)
- Stored in: `s3://netflixdataset-yourname` (S3 bucket manually created)
- Loaded into: `Snowflake` â†’ `NETFLIX_DB.RAW` tables

---

## ğŸ§Š Snowflake Configuration

Run the `instructions.sql` file to:

- Create `TRANSFORM` role
- Create `COMPUTE_WH` warehouse
- Create `NETFLIX_DB` database and `RAW` schema
- Create user: `YESHWANTH7482`
- Grant necessary permissions
- Load CSV data using Snowflake stage from S3

> â— **Note:** All credentials and keys (like AWS access keys, Snowflake passwords) have been scrubbed and excluded from this repo.

---

## âš™ï¸ Local Environment Setup

```bash
# Step 1: Create virtual env
python -m venv netflix_env
.
etflix_env\Scripts\ctivate  # On Windows

# Step 2: Install dbt and dependencies
pip install dbt-snowflake

# Step 3: Initialize dbt project
dbt init netflix_dbt_project
```

### ğŸ”‘ DBT Profile Configuration

Edit `C:\Users\<your_user>\.dbt\profiles.yml`

```yaml
netflix_dbt_project:
  outputs:
    dev:
      type: snowflake
      account: <your_account>
      user: "YESHWANTH7482"
      password: "<your_password>"
      role: TRANSFORM
      warehouse: COMPUTE_WH
      database: NETFLIX_DB
      schema: RAW
      threads: 4
  target: dev
```

---

## ğŸ—ï¸ dbt Modeling

### 1. ğŸ“¦ Staging Models (`models/staging/`)

Each raw table is referenced and aliased to standardized column names.

Examples:
```sql
SELECT movieId AS movie_id, title, genres
FROM {{ source('raw', 'raw_movies') }}
```

### 2. ğŸ“¦ Dimension Models (`models/dim/`)

Transformed views into clean dimension tables:
- `dim_movies`
- `dim_users`
- `dim_genome_tags`

### 3. ğŸ“¦ Fact Models (`models/fct/`)

Derived transaction-like models:
- `fct_genome_scores` (filtering relevance > 0)
- `fct_ratings` (incremental load on timestamp)

### 4. âš¡ Ephemeral Models

Used for joining temporary logic:
- `dim_movies_with_tags` (ephemeral)
- `ep_movie_with_tags` (CTE from above)

---

## ğŸ§ª Testing, Seeds, Snapshots

### âœ… Schema Tests:
- `not_null`, `relationships`, `expression_is_true`, etc.

### ğŸ“‚ Seeds:
Seed CSVs directly create Snowflake tables using:

```bash
dbt seed
```

### ğŸ•’ Snapshots:
Track history of slow-changing dimension data (like user-tagging behavior):

```bash
dbt snapshot
```

---

## ğŸ“Š dbt Commands

```bash
# Run all models
dbt run

# Run specific model
dbt run --select dim_movies

# Run incremental only
dbt run --full-refresh

# Run tests
dbt test

# Build everything (run + seed + snapshot + test)
dbt build

# Serve documentation
dbt docs generate
dbt docs serve
```

---

## ğŸ“š dbt Materializations (Quick Summary)

| Materialization | Description                                 | When to Use                         |
|------------------|---------------------------------------------|--------------------------------------|
| `view`           | Virtual model, always reflects latest data | Small data, quick refresh            |
| `table`          | Physical table in Snowflake                | Stable transformed data              |
| `incremental`    | Adds new rows only                         | Large tables with append-only logic  |
| `ephemeral`      | No physical table, used as CTE             | Intermediate logic, reused once      |

---

## ğŸ” Security Notes

- All passwords and AWS credentials are removed.
- Secrets like `AWS_SECRET_KEY`, `dbtPassword123` were **used locally only for demo** and should never be pushed to GitHub.

---

## ğŸ™ Credits

- **Darshil Parmar** â€” Original YouTube Masterclass
- You â€” For learning by building â¤ï¸

---

## ğŸ“ How to Clone and Use

```bash
git clone https://github.com/08YESHWANTH08/Netflix_Data_Analysis_ELT_Pipeline.git
cd Netflix_Data_Analysis_ELT_Pipeline

# (Optional) Setup virtual env
python -m venv netflix_env
.
etflix_env\Scripts\activate

pip install dbt-snowflake

# Add your profiles.yml in ~/.dbt/
# Add your own AWS S3 & Snowflake creds

dbt run
dbt test
dbt docs serve
```

---

Letâ€™s build more together! ğŸš€
